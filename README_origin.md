[![PyPI ç‰ˆæœ¬](https://badge.fury.io/py/kvpress.svg)](https://badge.fury.io/py/kvpress)
[![è®¸å¯è¯](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![Colab ç¤ºä¾‹ç¬”è®°æœ¬](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1JNvaTKuuAHrl49dYB9-mdEH_y52Ib-NP?usp=drive_link)
[![Hugging Face Space](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Space-blue)](https://huggingface.co/spaces/nvidia/kvpress)
[![åšå®¢](https://img.shields.io/badge/ğŸ¤—%20Hugging%20Face-Blog-blue)](https://huggingface.co/blog/nvidia/kvpress)
[![æ’è¡Œæ¦œ](https://img.shields.io/badge/ğŸ¤—%20HuggingFace-Leaderboard-orange)](https://huggingface.co/spaces/nvidia/kvpress-leaderboard)
[![è®ºæ–‡](https://img.shields.io/badge/ğŸ“„%20arXiv-Paper-red)](https://arxiv.org/abs/2510.00636v1)

![kvpress](kvpress.jpg)

é•¿ä¸Šä¸‹æ–‡ LLM çš„éƒ¨ç½²æˆæœ¬å¾ˆé«˜ï¼ŒåŸå› æ˜¯ Transformer çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜éšä¸Šä¸‹æ–‡é•¿åº¦çº¿æ€§å¢é•¿ã€‚ä¾‹å¦‚ï¼Œåœ¨ float16 ä¸‹ï¼ŒLlama 3.1â€‘70B å¤„ç† 100 ä¸‡ Token éœ€è¦çº¦ 330GB æ˜¾å­˜ã€‚KVPress åŸºäº ğŸ¤— transformers å®ç°äº†å¤šç§ KV ç¼“å­˜å‹ç¼©æ–¹æ³•åŠå…¶è¯„æµ‹ï¼Œæ—¨åœ¨ä¸ºç ”ç©¶è€…ä¸å¼€å‘è€…æä¾›ä¸€å¥—ç®€æ´ç»Ÿä¸€çš„å®ç°ä¸åŸºå‡†ã€‚

## å®‰è£…

```bash
pip install kvpress
```

æœ¬åœ°å¼€å‘å®‰è£…ï¼ˆåŒ…å«å…¨éƒ¨å¼€å‘ä¾èµ–ï¼Œä½¿ç”¨ uvï¼‰ï¼š

```bash
git clone https://github.com/NVIDIA/kvpress.git
cd kvpress
uv sync --all-groups
```
<details><summary>
é«˜çº§å®‰è£…è®¾ç½®
</summary>

å¯é€‰ä¾èµ–å»ºè®®ä½¿ç”¨ [uv](https://docs.astral.sh/uv/) å®‰è£…ï¼š

å¼€å¯ flashâ€‘attentionï¼š

```bash
git clone https://github.com/NVIDIA/kvpress.git
cd kvpress
uv sync --extra flash-attn
```

å®‰è£…è¯„æµ‹ç›¸å…³ä¾èµ–ï¼š

```bash
git clone https://github.com/NVIDIA/kvpress.git
cd kvpress
uv sync --extra eval
```
</details>

## ç”¨æ³•

KVPress æä¾›è‹¥å¹²åœ¨é¢„å¡«å……é˜¶æ®µå‹ç¼© KV ç¼“å­˜çš„â€œPressâ€ã€‚æ¯ç§ Press éƒ½æœ‰ä¸€ä¸ª `compression_ratio`ï¼ˆå‹ç¼©æ¯”ä¾‹ï¼‰ã€‚æ¨èé€šè¿‡è‡ªå®šä¹‰çš„ `KVPressTextGenerationPipeline` ä½¿ç”¨ï¼Œå®ƒåœ¨å¯¼å…¥ kvpress æ—¶ä¼šä»¥ `kv-press-text-generation` åç§°æ³¨å†Œä¸º transformers çš„ç®¡çº¿ï¼Œå¹¶è‡ªåŠ¨å¤„ç†èŠå¤©æ¨¡æ¿ä¸åˆ†è¯ï¼š

```python
from transformers import pipeline
from kvpress import ExpectedAttentionPress

device = "cuda:0"
model = "meta-llama/Llama-3.1-8B-Instruct"
model_kwargs = {"attn_implementation": "flash_attention_2"}
pipe = pipeline("kv-press-text-generation", model=model, device=device, model_kwargs=model_kwargs)

context = "ä¸€ä¸ªå¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼Œéœ€è¦å…ˆå‹ç¼©åå¤ç”¨"
question = "\nå…³äºè¯¥ä¸Šä¸‹æ–‡çš„é—®é¢˜"  # å¯é€‰

press = ExpectedAttentionPress(compression_ratio=0.5)
answer = pipe(context, question=question, press=press)["answer"]
```

ä¸Šè¿°ç¤ºä¾‹ä»…å¯¹ä¸Šä¸‹æ–‡è¿›è¡Œå‹ç¼©ï¼Œä¾¿äºé’ˆå¯¹ä¸åŒé—®é¢˜å¤ç”¨å‹ç¼©åçš„ç¼“å­˜ã€‚æ›´å®Œæ•´çš„ç¤ºä¾‹å¯å‚è€ƒ [Wikipedia æ¼”ç¤º](notebooks/wikipedia_demo.ipynb)ï¼ˆæ”¯æŒ Colabï¼‰ã€‚

<details><summary>
è§£ç æœŸå‹ç¼©ï¼ˆå®éªŒæ€§ï¼‰
</summary>

é»˜è®¤æƒ…å†µä¸‹ï¼ŒKVPress åœ¨é¢„å¡«å……é˜¶æ®µå‹ç¼©ã€‚æˆ‘ä»¬æä¾› `DecodingPress` åŒ…è£…å™¨ä»¥åœ¨è§£ç æœŸå‘¨æœŸæ€§å‹ç¼© KV ç¼“å­˜ï¼Œå¹¶å¯é€‰ç¼“å†²æœ€è¿‘çš„éšè—æ€ã€‚ä¸»è¦å‚æ•°ï¼š

- `base_press`ï¼šä»»æ„ ScorerPressï¼ˆå¦‚ `KnormPress`ã€`CriticalKVPress`ï¼‰
- `compression_interval`ï¼šå‹ç¼©é—´éš”æ­¥æ•°ï¼ˆé»˜è®¤ 10ï¼‰
- `target_size`ï¼šæ¯æ¬¡å‹ç¼©åç›®æ ‡ç¼“å­˜å¤§å°ï¼ˆé»˜è®¤ 1024ï¼‰
- `hidden_states_buffer_size`ï¼šå‹ç¼©å‰ç¼“å†²çš„éšè—æ€æ•°é‡ï¼ˆé»˜è®¤ 128ï¼Œæœ‰äº› Press å¯è®¾ä¸º 0ï¼‰

è§£ç å‹ç¼©ä½¿ç”¨ç›®æ ‡å¤§å°è€Œéå‹ç¼©æ¯”ä¾‹ï¼Œå³æ¯ `compression_interval` æ­¥å‹ä¸€æ¬¡ï¼Œè‡ªåŠ¨è®¡ç®—åˆ° `target_size` çš„æ¯”ä¾‹ã€‚

```python
from transformers import pipeline
from kvpress import KnormPress, DecodingPress

device = "cuda:0"
model = "meta-llama/Llama-3.1-8B-Instruct"
model_kwargs = {"attn_implementation": "flash_attention_2"}
pipe = pipeline("kv-press-text-generation", model=model, device=device, model_kwargs=model_kwargs)

decoding_press = DecodingPress(
    base_press=KnormPress(),
    compression_steps=10,
    token_buffer_size=512
)

context = "ä¸€ä¸ªéœ€è¦åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‹ç¼©çš„é•¿ä¸Šä¸‹æ–‡"
question = "è¯·åŸºäºè¯¥ä¸Šä¸‹æ–‡è®²ä¸€ä¸ªé•¿æ•…äº‹"
response = pipe(context, question=question, press=decoding_press)["answer"]
```

å¹¶éæ‰€æœ‰ Press éƒ½å®Œå…¨å…¼å®¹è§£ç å‹ç¼©ï¼Œç›®å‰ä¸»è¦æ”¯æŒ ScorerPress ä½œä¸ºåŸºåº•ã€‚

</details>

## å¯ç”¨çš„ Press

æ‰€æœ‰å½“å‰æ–¹æ³•å‡ä¸ºå…è®­ç»ƒï¼Œç»§æ‰¿è‡ª `BasePress`ï¼ˆè§ `kvpress/presses/base_press.py`ï¼‰ã€‚

åŸºäºæ‰“åˆ†çš„å‹ç¼©ï¼ˆç»§æ‰¿ `ScorerPress`ï¼Œè§ `kvpress/presses/scorer_press.py`ï¼‰ï¼š

- `RandomPress`ï¼šéšæœºæ‰“åˆ†
- `KnormPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2406.11430ï¼‰ï¼šKey é€†èŒƒæ•°
- `SnapKVPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2404.14469ï¼‰ï¼šè¿‘æœŸ Query çš„å¹³å‡æ³¨æ„åŠ›
- `ExpectedAttentionPress`ï¼ˆç¬”è®°æœ¬ï¼šnotebooks/expected_attention.ipynbï¼‰ï¼šåŸºäºæœªæ¥ Query åˆ†å¸ƒçš„æœŸæœ›æ³¨æ„åŠ›
- `StreamingLLMPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2309.17453ï¼‰ï¼šä¿ç•™å¼€å¤´å’Œè¿‘æœŸ Token
- `TOVAPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2401.06104ï¼‰ï¼šæœ€åä¸€ä¸ª Query çš„æ³¨æ„åŠ›ï¼ˆè·¨å¤´å¹³å‡ï¼‰
- `ObservedAttentionPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2306.14048ï¼‰ï¼šé¢„å¡«å……é˜¶æ®µçš„è§‚æµ‹æ³¨æ„åŠ›
- `QFilterPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2503.02812ï¼‰ï¼šå°† Key æŠ•å½±åˆ° Query çš„ä¸» SVD åˆ†é‡ä»¥è¿‘ä¼¼æ³¨æ„åŠ›
- `PyramidKVPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2406.02069ï¼‰ï¼šé‡‘å­—å¡”å¼åˆ†é…ç¼“å­˜é¢„ç®—
- `LagKVPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2504.04704ï¼‰ï¼šåˆ©ç”¨ KV æ»åä¿¡æ¯ï¼Œå… Queryã€å…æ³¨æ„åŠ›ã€å…¼å®¹ flashâ€‘attn
- `KeyDiffPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2504.15364ï¼‰ï¼šåŸºäº Key ç›¸ä¼¼åº¦æ·˜æ±°
- `NonCausalAttnPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2507.08143ï¼‰ï¼šåŸºäºéå› æœåˆ†å—æ³¨æ„åŠ›æ‰“åˆ†
- `LeverageScorePress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2507.08143ï¼‰ï¼šè¿‘ä¼¼ç»Ÿè®¡æ æ†åˆ†ï¼ˆä¿ç•™ Key ç©ºé—´çš„ç¦»ç¾¤ç‚¹ï¼‰
- `CompactorPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2507.08143ï¼‰ï¼šåœ¨ `compression_ratio` ä¸Šèåˆéå› æœæ³¨æ„ä¸æ æ†åˆ†
- `CURPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2509.15038ï¼‰ï¼šåŸºäº CUR åˆ†è§£çš„è¿‘ä¼¼æ æ†åˆ†å‹ç¼©

å…¶ä»–æ€è·¯ï¼š
- `ThinKPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/pdf/2407.21018ï¼‰ï¼šæŒ‰é€šé“æ³¨æ„åŠ›å‹ Key çš„ç»´åº¦
- `SimLayerKVPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2410.13846ï¼‰ï¼šè¯†åˆ«â€œæ‡’æƒ°å±‚â€ï¼Œå¯¹å…¶åº”ç”¨ StreamingLLM
- `DuoAttentionPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2410.10819ï¼‰ï¼šå°†å¤´åˆ’åˆ†ä¸ºæ£€ç´¢å¤´ä¸æµå¼å¤´
- `FinchPress`ï¼ˆè®ºæ–‡ï¼šhttps://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00716/125280ï¼‰ï¼šåŠ¨æ€çª—å£ + Key é‡æ—‹è½¬ï¼Œç±»ä¼¼ SnapKV
- `KVzipPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2505.23416ï¼‰ï¼šé€šè¿‡ä¸Šä¸‹æ–‡é‡å»ºè¯†åˆ«å†—ä½™ KVï¼Œè¿‘æ— æŸä½†éœ€è¦å¤šæ¬¡å‰å‘

ç»„åˆ/åŒ…è£…ç±»ï¼š
- `AdaKVPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2407.11550ï¼‰ï¼šè·¨å¤´ä¿ç•™é«˜åˆ†ï¼ŒæŒ‰å¤´å‹ç¼©
- `PerLayerCompressionPress`ï¼šåˆ†å±‚è®¾ç½®å‹ç¼©æ¯”ä¾‹ï¼ˆå®éªŒæ€§ï¼‰
- `ComposedPress`ï¼šä¸²è”å¤šä¸ª Press çš„é’©å­
- `KeyRerotationPress`ï¼šå¯¹è¢«å‰ªçš„ Key é‡æ–°æ—‹è½¬ä»¥ä¿æŒ RoPE è¿ç»­
- `ChunkKVPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2502.00299ï¼‰ï¼šæŒ‰è¯­ä¹‰å—é€‰æ‹©ä¿ç•™ç‰‡æ®µ
- `ChunkPress`ï¼ˆè®ºæ–‡ï¼šhttps://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00716/125280ï¼‰ï¼šæŒ‰åˆ†å—åˆ†åˆ«å‹ç¼©ï¼Œæå‡é•¿åºåˆ—å‡åŒ€æ€§
- `CriticalKVPress` / `CriticalAdaKVPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2502.03805ï¼‰ï¼šç»“åˆ `Wo @ V` çš„ L1 èŒƒæ•°ä¸ä¸¤é˜¶æ®µé€‰æ‹©
- `BlockPress`ï¼ˆè®ºæ–‡ï¼šhttps://arxiv.org/abs/2504.15364ï¼‰ï¼šåˆ†å—è¿­ä»£å‹ç¼©
- `DecodingPress`ï¼šè§£ç æœŸå‹ç¼©
- `PrefillDecodingPress`ï¼šåŒæ—¶æ”¯æŒé¢„å¡«å……ä¸è§£ç æœŸå‹ç¼©

æ›´å¤š KV ç¼“å­˜å‹ç¼©æ–¹æ³•å¯å‚è€ƒï¼š
https://github.com/October2001/Awesome-KV-Cache-Compression
https://github.com/HuangOwen/Awesome-LLM-Compression?tab=readme-ov-file#kv-cache-compression

## è¯„æµ‹

æˆ‘ä»¬æä¾›è¯„æµ‹ CLIï¼ˆ`evaluation/evaluate.py`ï¼‰ä»¥åœ¨å¤šç§é•¿ä¸Šä¸‹æ–‡åŸºå‡†ä¸Šæµ‹è¯•ä¸åŒ Press çš„è¡¨ç°ã€‚

- å‡†ç¡®ç‡ï¼šç›´æ¥åœ¨ RULERã€LongBenchã€ZeroScrolls ç­‰æ•°æ®é›†ä¸Šè¯„æµ‹ï¼›ç»“æœä¿å­˜åœ¨ `results/...`ã€‚
- é€Ÿåº¦ä¸æ˜¾å­˜ï¼šå¯å‚è€ƒ `notebooks/speed_and_memory.ipynb` ï¼›æˆ–ä½¿ç”¨ä¸‹è¿° PPL/åŠ é€Ÿè„šæœ¬è¿›è¡Œåº¦é‡ã€‚

æ’è¡Œæ¦œå¹³å‡è¡¨ç°ï¼ˆRULER 4k ä¸Šä¸‹æ–‡ï¼‰ï¼š

<p>
  <img src="leaderboard_plot_score.png" alt="Leaderboard">
</p>

### åœ¨ PGâ€‘19 ä¸ WikiText ä¸Šè¿›è¡Œ PPL ä¸åŠ é€Ÿè¯„æµ‹

æ–°å¢è„šæœ¬ï¼š`evaluation/perplexity.py`

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -e .
pip install datasets
```

WikiTextâ€‘103 PPLï¼ˆåŸºçº¿ï¼‰ï¼š

```bash
python evaluation/perplexity.py --model EleutherAI/pythia-70m \
  --dataset wikitext --subset wikitext-103-v1 --press no_press --attn_implementation eager
```

WikiTextâ€‘103 åŠ é€Ÿï¼ˆå‹ç¼©ç¤ºä¾‹ï¼‰ï¼š

```bash
python evaluation/perplexity.py --model EleutherAI/pythia-70m \
  --dataset wikitext --subset wikitext-103-v1 \
  --press snapkv --compression_ratio 0.5 --attn_implementation eager
```

PGâ€‘19 è¶…é•¿æ–‡æœ¬ï¼ˆå–å•ä¸€æ ·æœ¬ï¼‰åŸºçº¿ä¸åŠ é€Ÿï¼š

```bash
# åŸºçº¿
python evaluation/perplexity.py --model EleutherAI/pythia-70m --dataset pg19 --sample_idx 0 --press no_press

# å‹ç¼©ï¼ˆç¤ºä¾‹ï¼‰
python evaluation/perplexity.py --model EleutherAI/pythia-70m --dataset pg19 --sample_idx 0 \
  --press knorm --compression_ratio 0.5
```

è„šæœ¬ä¼šè¾“å‡º/ä¿å­˜ PPLã€ç”Ÿæˆé€Ÿåº¦ï¼ˆtok/sï¼‰ã€å³°å€¼æ˜¾å­˜ã€ä¸Šä¸‹æ–‡ Token æ•°ç­‰æŒ‡æ ‡ã€‚

## é‡åŒ–

æ”¯æŒé€šè¿‡ transformers çš„ `QuantizedCache` è¿›è¡Œ KV ç¼“å­˜é‡åŒ–ï¼ˆå‚è€ƒ HF åšæ–‡ï¼‰ã€‚ç”¨æ³•ç¤ºä¾‹ï¼š

```python
from transformers import QuantizedCacheConfig, QuantoQuantizedCache

config = QuantizedCacheConfig(nbits=4)
cache = QuantoQuantizedCache(config)

pipe(..., cache=cache)
```

é»˜è®¤ä½¿ç”¨ `DynamicCache`ï¼ˆä¸é‡åŒ–ï¼‰ã€‚å¦‚éœ€ä½¿ç”¨ `QuantizedCache`ï¼Œè¯·å…ˆå®‰è£… `optimum-quanto` ç­‰ä¾èµ–ã€‚

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®æ–°æ–¹æ³•ã€‚æ–°å¢ Press å¯å‚è€ƒ `notebooks/new_press.ipynb` çš„åˆ†æ­¥æ•™ç¨‹åæäº¤ PRã€‚

## å¼•ç”¨

```bibtex
@article{devoto2025expectedattention,
  title={Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution},
  author={Devoto, Alessio and Jeblick, Maximilian and J{\'e}gou, Simon},
  journal={arXiv preprint arXiv:2510.00636},
  year={2025},
  url={https://arxiv.org/abs/2510.00636}
}
```

## å¸¸è§é—®é¢˜

<details><summary>
æ”¯æŒçš„æ¨¡å‹æœ‰å“ªäº›ï¼Ÿ
</summary>

éƒ¨åˆ† Press ä¾èµ–å…·ä½“æ¶æ„ï¼ˆå¦‚ `ExpectedAttentionPress`ã€`SnapKVPress`ï¼‰ï¼Œå› æ­¤å¯èƒ½åªåœ¨éƒ¨åˆ†æ¨¡å‹ä¸Šå·¥ä½œã€‚å½“å‰å·²æµ‹è¯•æ”¯æŒï¼š`LlamaForCausalLM`ã€`MistralForCausalLM`ã€`Phi3ForCausalLM`ã€`Qwen2ForCausalLM`ã€`Qwen3ForCausalLM`ã€`Gemma3ForConditionalGeneration`ã€‚æœ¬ä»“åº“å·²é€‚é… `GPTNeoXForCausalLM`ï¼Œå¯ç”¨äº Pythiaâ€‘70Mã€‚
</details>

<details><summary>
å¦‚ä½•ä½¿ç”¨å¤š GPU æ¨ç†ï¼Ÿ
</summary>

KVPress é€šè¿‡ [accelerate](https://huggingface.co/docs/accelerate/en/index) æ”¯æŒå¤š GPUï¼š

```python
pipe = pipeline("kv-press-text-generation", model=model, device_map="auto")
```

</details>

<details><summary>
å‹ç¼©å¸¦æ¥çš„å†…å­˜ä¸ååæå‡ï¼Ÿ
</summary>

æ˜¾å­˜å ç”¨çº¦å‡å°‘ä¸º `compression_ratio * kv_cache_size`ã€‚ç”±äº KV ç¼“å­˜å˜å°ï¼Œè§£ç é€Ÿåº¦é€šå¸¸æå‡ã€‚å¯ä½¿ç”¨ `notebooks/speed_and_memory.ipynb` æˆ– `evaluation/perplexity.py` è¿›è¡Œåº¦é‡ã€‚
</details>


<details> <summary> 

### How does a press work ? </summary>

A press registers a forward hook (`press.forward_hook` method) to each attention layer during the pre-filling phase. Registration can be applied using the press as a context manager (`press.__call__` method):

```python
import torch
from transformers import AutoModelForCausalLM
from kvpress import KnormPress

device = "cuda:0"
ckpt = "meta-llama/Meta-Llama-3.1-8B-Instruct"
model = AutoModelForCausalLM.from_pretrained(ckpt).to(device)
press = KnormPress(compression_ratio=0.4)

inputs = model.dummy_inputs["input_ids"].to(device)

with torch.no_grad():
    print(model(inputs).past_key_values[0][0].shape)
    # torch.Size([3, 8, 5, 128])
    
with torch.no_grad(), press(model):
    print(model(inputs).past_key_values[0][0].shape)
    # torch.Size([3, 8, 3, 128])
```
</details>

<details><summary> 

### Why not using model.generate ? 
</summary>

In fact you can use `model.generate` with a press by using the press as a context manager:

```python
with press(model):
    outputs = model.generate(inputs)
```

However, the `generate` method does not allow to exclude the question from the compression, which would artificially favors methods such as SnapKV. Ideally, we want a compression method that works whatever comes after the context (_e.g._ for use cases such as chat or document question answering). Finally the `generate` method does not allow to provide generation for multiple questions at once.

</details>



<details><summary> 

### Can I combine compression during prefilling and decoding ? 
</summary>


Combines separate presses for prefilling and decoding phases.

**Parameters:**
- `prefilling_press`: Press used during prefill phase
- `decoding_press`: Press used during decoding phase

## Usage Examples

### Basic Decoding Compression

```python
from transformers import pipeline
from kvpress import KnormPress
from kvpress import DecodingPress

# Initialize the pipeline
device = "cuda:0"
model = "meta-llama/Llama-3.1-8B-Instruct"
model_kwargs = {"attn_implementation": "flash_attention_2"}
pipe = pipeline("kv-press-text-generation", model=model, device=device, model_kwargs=model_kwargs)

# Create a decoding press that compresses every 10 steps to 512 tokens
decoding_press = DecodingPress(
    base_press=KnormPress(),
    compression_steps=10,
    token_buffer_size=512
)

# Use with pipeline
context = "A very long text you want to compress during generation"
question = "Tell me a long story about this context"
response = pipe(context, question=question, press=decoding_press)["answer"]
```

### Combined Prefill + Decoding Compression

```python
from transformers import pipeline
from kvpress import CriticalKVPress, KnormPress
from kvpress import DecodingPress, PrefillDecodingPress

# Initialize the pipeline
device = "cuda:0"
model = "meta-llama/Llama-3.1-8B-Instruct"
model_kwargs = {"attn_implementation": "flash_attention_2"}
pipe = pipeline("kv-press-text-generation", model=model, device=device, model_kwargs=model_kwargs)

# Different strategies for prefill vs decoding
prefill_press = CriticalKVPress(KnormPress())
decoding_press = DecodingPress(
    base_press=KnormPress(compression_ratio=0.2),
    compression_steps=5,
    token_buffer_size=256
)

# Combine them
combined_press = PrefillDecodingPress(
    prefilling_press=prefill_press,
    decoding_press=decoding_press
)

context = "A very long context that will be compressed during prefill"
question = "Generate a detailed analysis that will be compressed during decoding"
response = pipe(context, question=question, press=combined_press)["answer"]
```
